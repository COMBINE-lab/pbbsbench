This is the readme file for the PPoPP  artifact evaluation.

GETTING STARTED:

The submitted directory should be self contained.   It can also be downloaded from github using:

> git clone https://github.com/cmuparlay/pbbsbench.git
> cd pbbsbench
> git submodule init
> git submodule update

Requirements:

- some linux based OS (we tested on ubuntu)

* For the small inputs
- 12GB of memory
- 10GB of free disk space

* For the large inputs
- 64GB of memory
- 90GB of free disk space (generates large temporary files)

Not required, but will give better performance

- jemalloc  (only gives slight performance improvement)
- 20+ cores (the more cores the faster
- numactl installed (if this is not installed you need to run "./runall -nonuma")

The command ./runall will run all the benchmarks reported but will take a couple hours.
For a faster run, try:

  ./runall -par -small

This only runs the parallel benchmarks, which run much faster, and on
significantly smaller input data (an order of magnitude smaller for some benchmarks).
This runs in 12 minutes on a 20 core (40 hyperthread) machine.

You can also test individual benchmarks.   For example, you can test the
parallel comparison sort using:

  ./runall -only comparisonSort/sampleSort

This will run the parallel sampleSort on the default (full sized) inputs.  
The call

  ./runall -small -only comparisonSort/sampleSort

will run it on the smaller inputs.  More details on arguments are
given below.

STEP-BY-STEP

Here we describe more options to run all or some of the the benchmarks.

The ./runall has the following options which can be extracted by using
"./runall -h".  

  -scale    : this runs it on a range of different thread counts up the the number of threads on the machine
  -small    : runs tests on smaller inputs (calls ./testInput_small instead of ./testInput).
  -par      : only run benchmarks that are parallel (saves time)
  -only <name>   : only run a particular benchmark
  -nonuma   : don't use numactl
  -nocheck  : don't check correctness of results (saves time)

For the -only option use the path to the implementation, e.g.

  ./runall -only comparisonSort/sampleSort
  
The benchmarks themselves are organized hierarchically.  At the top
level are the following benchmarks:

  breadthFirstSearch
  BWDecode
  classify
  comparisonSort
  convexHull
  delaunayRefine
  delaunayTriangulation
  histogram
  index
  integerSort
  longestRepeatedSubstring
  maximalIndependentSet
  maximalMatching
  minSpanningForest
  nBody
  nearestNeighbors
  rayCast
  rangeQuery2d
  removeDuplicates	
  spanningForest
  suffixArray
  wordCounts

Within each benchmarks are the implementations.  The benchmarks also
have some directories shared across implementations.  In particular
each one has a directory called "bench" which contains the driver
code, testing code, and specification of default inputs.  There is
also a directory for generating the data.

Within each implementation directory, you can run "make" to make the
executable, and then run "./testInputs" to run the benchmarks.  These
are run automatically by the ./runall script.  On a machine with
multiple chips, using "numactl -i all ./testInputs" will give better
results.  "./testInputs_small" will use the smaller inputs.

The "testInputs" script has several options including:

  -x : do not check the output
  -r <count>  : number of rounds to use
  -p <count>  : number of threads to use

The actual inputs are specified in the script and can be changed if desired.

There are many more implementations than ./runall will run.  Feel free
to run any of them.

LIST OF CLAIMS

The main claims from the paper are

  1) a set of 23 benchmarks.  One is missing due to an update in the
underlying library that broke it at the last minute.  We plan to add
it back (or drop from the paper in the unlikely event we can't get it
to work).

  2) multiple implementations for most of them.  Sorting, for example,
  has 7 implementations (the most).   All have a parallel implementation.

  3) test, timing and data generation harnesses for each of them.  The
  test and timing harnesses are in the bench subdirectory for each
  benchmark.  The data generation is in the *data subdirectory (where
  * depends on the benchmark).

  4) default data sets for each benchmark.  We supply both a small and
  large (default) set (specified in the testInput and testInput_small scripts).

  5) The ability to test across different numbers of threads.

We believe the benchmarks show all these (except the caveat of the
missing Set Cover benchmark).
